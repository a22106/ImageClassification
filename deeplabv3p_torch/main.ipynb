{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\imgseg\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "import network\n",
    "from network.deeplabv3 import DeepLabv3Plus\n",
    "import utils\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils import data\n",
    "#from datasets import VOCSegmentation, Cityscapes\n",
    "#from utils import ext_transforms as et\n",
    "#from metrics import StreamSegMetrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "#from utils.visualizer import Visualizer\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import yaml\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import yaml\n",
    "\n",
    "def load_yaml(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_harbor_idx(data_path, train = True, is_label = False, label_num = 5):\n",
    "    if train:\n",
    "        if is_label:\n",
    "            classes = ['ship', 'container_truck', 'forklift', 'reach_stacker']\n",
    "            image_path = glob(os.path.join(root, 'train', 'labeled_images', '*.jpg'))\n",
    "            image_idx_list = list(map(lambda x : x.split('/')[-1].split('.')[0], image_path))\n",
    "            train_idx = []\n",
    "            valid_idx = []\n",
    "            for c in classes:\n",
    "                matched_idx = [i for i in image_idx_list if c in i]\n",
    "                train_idx.extend(matched_idx[label_num:])\n",
    "                valid_idx.extend(matched_idx[:label_num])\n",
    "            return train_idx, valid_idx\n",
    "        else:\n",
    "            image_path = glob(os.path.join(root, 'train', 'unlabeled_images', '*.jpg'))\n",
    "            train_idx = list(map(lambda x: x.split('/')[-1].split('.')[0], image_path))\n",
    "            return train_idx\n",
    "    else:\n",
    "        image_path = glob(os.path.join(root, 'test', 'images', '*.jpg'))\n",
    "        test_idx = list(map(lambda x: x.split('/')[-1].split('.')[0], image_path))\n",
    "        return test_idx\n",
    "\n",
    "\n",
    "\n",
    "class BuildDataLoader:\n",
    "    def __init__(self, num_labels, dataset_path, batch_size):\n",
    "        self.data_path = dataset_path\n",
    "        self.im_size = [513, 513]\n",
    "        self.crop_size = [430, 430]\n",
    "        self.num_segments = 5\n",
    "        self.scale_size = (0.5, 1.5)\n",
    "        self.batch_size = batch_size\n",
    "        self.train_l_idx, self.valid_l_idx = get_harbor_idx(self.data_path, train=True, is_label=True, label_num=num_labels)\n",
    "        self.train_u_idx = get_harbor_idx(self.data_path, train=True, is_label=False)\n",
    "        self.test_idx = get_harbor_idx(self.data_path, train=False)\n",
    "\n",
    "        if num_labels == 0:  # using all data\n",
    "            self.train_l_idx = self.train_u_idx\n",
    "\n",
    "    def build(self, supervised=False):\n",
    "        train_l_dataset = BuildDataset(self.data_path, self.train_l_idx,\n",
    "                                       crop_size=self.crop_size, scale_size=self.scale_size,\n",
    "                                       augmentation=True, train=True, is_label=True)\n",
    "        train_u_dataset = BuildDataset(self.data_path, self.train_u_idx,\n",
    "                                       crop_size=self.crop_size, scale_size=(1.0, 1.0),\n",
    "                                       augmentation=False, train=True, is_label=False)\n",
    "        valid_l_dataset = BuildDataset(self.data_path, self.valid_l_idx,\n",
    "                                       crop_size=self.crop_size, scale_size=self.scale_size,\n",
    "                                       augmentation=False, train=True, is_label=True)\n",
    "        test_dataset    = BuildDataset(self.data_path, self.test_idx,\n",
    "                                       crop_size=self.im_size, scale_size=(1.0, 1.0),\n",
    "                                       augmentation=False, train=False, is_label=True)\n",
    "\n",
    "        if supervised:  # no unlabelled dataset needed, double batch-size to match the same number of training samples\n",
    "            self.batch_size = self.batch_size * 2\n",
    "\n",
    "        num_samples = self.batch_size * 200  # for total 40k iterations with 200 epochs\n",
    "        # num_samples = self.batch_size * 2\n",
    "        train_l_loader = torch.utils.data.DataLoader(\n",
    "            train_l_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=sampler.RandomSampler(data_source=train_l_dataset,\n",
    "                                          replacement=True,\n",
    "                                          num_samples=num_samples),\n",
    "            drop_last=True,)\n",
    "\n",
    "\n",
    "        valid_l_loader = torch.utils.data.DataLoader(\n",
    "            valid_l_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=sampler.RandomSampler(data_source=valid_l_dataset,\n",
    "                                          replacement=True,\n",
    "                                          num_samples=num_samples),\n",
    "            drop_last=True,)\n",
    "\n",
    "        if not supervised:\n",
    "            train_u_loader = torch.utils.data.DataLoader(\n",
    "                train_u_dataset,\n",
    "                batch_size=self.batch_size,\n",
    "                sampler=sampler.RandomSampler(data_source=train_u_dataset,\n",
    "                                              replacement=True,\n",
    "                                              num_samples=num_samples),\n",
    "                drop_last=True,)\n",
    "\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        if supervised:\n",
    "            return train_l_loader, valid_l_loader, test_loader\n",
    "        else:\n",
    "            return train_l_loader, train_u_loader, valid_l_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "from collections import namedtuple\n",
    "\n",
    "class Comp_dataset(data.Dataset):\n",
    "    DatasetClass = namedtuple('DataClass', ['name', 'id'])\n",
    "    classes = [\n",
    "        DatasetClass('unlabeled', 0),\n",
    "        DatasetClass('container_truck', 1),\n",
    "        DatasetClass('forklift', 2),\n",
    "        DatasetClass('reach_stacker', 3),\n",
    "        DatasetClass('ship', 4)\n",
    "    ]\n",
    "\n",
    "    id_to_train_id = np.array([c.train_id for c in classes])\n",
    "\n",
    "    def __init__(self, root, transform = None):\n",
    "        # Base directory\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.image_dir = os.path.join(self.root, 'data')\n",
    "        self.num_classes = 5\n",
    "        self.transform = transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    train_transform = extcompose([])\n",
    "\n",
    "    train_dataset = Comp_dataset(root=, split=, transform=)\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_yaml(path, obj):\n",
    "\twith open(path, 'w') as f:\n",
    "\t\tyaml.dump(obj, f, sort_keys=False)\n",
    "\t\t\n",
    "\n",
    "def load_yaml(path):\n",
    "\twith open(path, 'r', encoding='utf-8') as f:\n",
    "\t\treturn yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from modeling.backbone import resnet\n",
    "model_urls = {'resnet101': \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\"}\n",
    "\n",
    "# Conv parameters = Kernel size x Kernel Size x Input Channel x Output Channel\n",
    "# BottleNeck is 1x1 Conv: 1 x 1 x Input Channel x Output Channel\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, inplanes, planes, stride = 1, dilation=1, downsample=None, BatchNorm=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = BatchNorm(planes)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            planes, planes, kernel_size=3, stride=stride, dilation=dilation,\n",
    "            padding=dilation, bias=False,\n",
    "        )\n",
    "        self.bn2 = BatchNorm(planes)\n",
    "        self.conv3 = nn.Conv2d(planes,planes*4, kernel_size=1, bias=False)\n",
    "        self.bn3 = BatchNorm(planes*4)\n",
    "        self.relu = nn.ReLU(inplace= True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual # f(x) + x\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Resnet class. make layers\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, output_stride, BatchNorm, pretrained = True, url = None):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        blocks = [1, 2, 4]\n",
    "        if output_stride == 16:\n",
    "            strides = [1, 2, 2, 1]\n",
    "            dilations = [1, 1, 1, 2]\n",
    "        elif output_stride == 8:\n",
    "            strides = [1, 2, 1, 1]\n",
    "            dilations = [1, 1, 2, 4]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    \n",
    "        # Modules\n",
    "        self.conv1 = nn.Conv2d(3, 64, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = BatchNorm(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], stride = strides[0],\n",
    "                                        dilation = dilations[0], BatchNorm=BatchNorm,\n",
    "        )\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride = strides[1],\n",
    "                                        dilation= dilations[1], BatchNorm=BatchNorm)\n",
    "        self.layer3 = self._make_layer(block, 128, layers[2], stride = strides[2],\n",
    "                                        dilation= dilations[2], BatchNorm=BatchNorm)\n",
    "        self.layer4 = self._make_layer(block, 128, layers[2], stride = strides[2],\n",
    "                                        dilation= dilations[2], BatchNorm=BatchNorm)\n",
    "\n",
    "        \n",
    "        self._init_weight()\n",
    "        if pretrained:\n",
    "            print(\"load pretrained RESNET model's weight\")\n",
    "            self._load_pretrained_model(url)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, BatchNorm=None):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            #self.inplanes: input_channel\n",
    "            downsample = nn.Sequential(                \n",
    "                nn.Conv2d(\n",
    "                    self.inplanes, planes*block.expension,\n",
    "                    kernel_size=1, stride=stride,bias=False,),\n",
    "                BatchNorm(planes*block.expansion),\n",
    "            )\n",
    "            \n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(self.inplanes, planes, stride, dilation, downsample, BatchNorm=BatchNorm)\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(self.inplanes, planes, dilation=dilation, BatchNorm=BatchNorm)\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_MG_unit(\n",
    "        self, block, planes, blocks, stride=1, dilation=1, BatchNorm=None\n",
    "    ):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    self.inplanes, planes*block.expansion, kernel_size=1,\n",
    "                    stride = stride, bias=False,\n",
    "                ),\n",
    "                BatchNorm(planes * block.expansion),\n",
    "            )\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes, planes, stride,dilation=blocks[0]*dilation,\n",
    "                downsample=downsample,BatchNorm = BatchNorm,\n",
    "            )\n",
    "        )\n",
    "        self.inplanes =planes *block.expansion\n",
    "        for i in range(1, len(blocks)):\n",
    "            layers.append(\n",
    "                block(self.inplanes,planes,stride = 1,\n",
    "                      dilation=blocks[i]*dilation, BatchNorm=BatchNorm,)\n",
    "            )\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    @torch.cuda.amp.autocast()\n",
    "    def forward(self, input, intercept=False):\n",
    "        x = self.conv1(input) # Conv2d(3, 64)\n",
    "        x = self.bn1(x) # Batchnorm(64)\n",
    "        x = self.relu(x) # ReLU\n",
    "        x = self.maxpool(x) # MaxPool2d\n",
    "\n",
    "        x_1 = self.layer1(x)\n",
    "        x_2 = self.layer2(x_1)\n",
    "        x_3 = self.layer3(x_2)\n",
    "        x_4 = self.layer4(x_3)\n",
    "        if intercept is True:\n",
    "            return x_1, x_3, x_4\n",
    "        return x_4, x_1\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2.0 / n))\n",
    "            elif isinstance(m, SynchronizedBatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "def ResNet101(output_stride, BatchNorm, pretrained=True):\n",
    "    r'''Construct a ResNet-101 model\n",
    "    Args:\n",
    "        pretrained(bool): If True, returns a model pre-trained on ImageNet\n",
    "    '''\n",
    "    print(\"Using Resnet101 as a Backbone Network\")\n",
    "    model = ResNet(\n",
    "        Bottleneck, [3, 4, 23, 3], output_stride, BatchNorm,\n",
    "        pretrained=pretrained, url=model_urls['resnet101'],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\imgseg\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    }
   ],
   "source": [
    "from models.sync_batchnorm.batchnorm import SynchronizedBatchNorm2d\n",
    "from models.assp import build_aspp\n",
    "from models.decoder import build_decoder\n",
    "\n",
    "class classifier(nn.Module):\n",
    "    def __init__(self, num_classes = 5, out_dim = 256):\n",
    "        super(classifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(out_dim, out_dim, kernel_size=1, stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_dim, num_classes, kernel_size=1, stride=1),\n",
    "        )\n",
    "\n",
    "    @torch.cuda.amp.autocast()\n",
    "    def forward(self, x, expand = None):\n",
    "        cls = self.classifier(x)\n",
    "        return cls # b x num_class\n",
    "\n",
    "class Projector(nn.Module):\n",
    "    def __init__(self, out_dim = 256):\n",
    "        super(Projector, self).__init__()\n",
    "\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Conv2d(out_dim, out_dim, kernel_size=1, stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_dim, 128, kernel_size=1, stride=1),\n",
    "        )\n",
    "    \n",
    "    @torch.cuda.amp.autocast()\n",
    "    def forward(self, x):\n",
    "        x = self.projector(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeepLab_temp(nn.Module):\n",
    "    def __init__(self, output_stride = 16):\n",
    "        super(DeepLab_temp, self).__init__()\n",
    "        BatchNorm = SynchronizedBatchNorm2d\n",
    "\n",
    "        self.backbone = ResNet101(output_stride, BatchNorm)\n",
    "    \n",
    "    @torch.cuda.amp.autocast()\n",
    "    def forward(self, input):\n",
    "        x, low_level_feat = self.backbone(input)\n",
    "        return x, low_level_feat\n",
    "\n",
    "    def get_backbone_params(self):\n",
    "        modules = [self.backbone]\n",
    "        for i in range(len(modules)):\n",
    "            #nn.Module.named_modules: returns an iterator over all modules in the network\n",
    "            for m in modules[i].named_modules():\n",
    "                if(isinstance(m[1], nn.Conv2d)\n",
    "                   or isinstance(m[1], nn.BatchNorm2d)\n",
    "                   or isinstance(m[1], nn.BatchNorm2d)\n",
    "                ):\n",
    "                    for p in m[1].parameters():\n",
    "                        if p.requires_grad:\n",
    "                            yield p\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, backbone=\"resnet101\", output_stride=16, num_classes=256,num_cls = 5\n",
    "    ):\n",
    "        BatchNorm = SynchronizedBatchNorm2d\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "        self.aspp = build_aspp(backbone, output_stride, BatchNorm)\n",
    "        self.decoder = build_decoder(num_classes, backbone, BatchNorm)\n",
    "        self.cls = classifier(num_classes=num_cls)\n",
    "        self.proj = Projector()\n",
    "\n",
    "    @torch.cuda.amp.autocast()\n",
    "    def forward(self, x, low_level_feat, gt_mode = False):\n",
    "        x = self.aspp(x, gt_mode)\n",
    "        x_ = self.decoder(x, low_level_feat, gt_mode)\n",
    "        cls = self.cls(x_)\n",
    "        proj = self.proj(x_)\n",
    "        return cls, proj\n",
    "    \n",
    "    def get_other_params(self):\n",
    "        modules = [self.aspp, self.decoder, self.cls, self.proj]\n",
    "        for i in range(len(modules)):\n",
    "            for m in modules[i].named_modules():\n",
    "                if(isinstance(m[1], nn.Conv2d)\n",
    "                   or isinstance(m[1], nn.BatchNorm2d)\n",
    "                   or isinstance(m[1], nn.BatchNorm2d)\n",
    "                ):\n",
    "                    for p in m[1].parameters():\n",
    "                        if p.requires_grad:\n",
    "                            yield p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Root directory\n",
    "PROJECT_DIR = os.path.join('C:/Users/Admin/OneDrive/C Documents/GitHub/ImageClassification/deeplabv3p_torch')\n",
    "\n",
    "# Load config\n",
    "config_path = os.path.join(PROJECT_DIR, 'config', 'train_config.yml')\n",
    "config = load_yaml(config_path)\n",
    "\n",
    "# # Train Serial\n",
    "# kst = timezone(timedelta(hours=9))\n",
    "# train_serial = datetime.now(tz=kst).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# # Recorder directory\n",
    "# RECORDER_DIR = os.path.join(PROJECT_DIR, 'results', 'train', train_serial)\n",
    "# os.makedirs(RECORDER_DIR, exist_ok=True)\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = os.path.join(\"C:/Users/Admin/OneDrive/C Documents/GitHub/aivill_seg/dataset\")\n",
    "\n",
    "# GPU\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = config['gpu']\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "# Setup random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Admin/OneDrive/C Documents/GitHub/aivill_seg/dataset\n"
     ]
    }
   ],
   "source": [
    "print(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Resnet101 as a Backbone Network\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'kernel_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\OneDrive\\C Documents\\GitHub\\ImageClassification\\deeplabv3p_torch\\main.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000015?line=0'>1</a>\u001b[0m num_classes \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000015?line=1'>2</a>\u001b[0m \u001b[39m####\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000015?line=2'>3</a>\u001b[0m \u001b[39m#set up model (all models are 'constructed at network.modeling)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000015?line=3'>4</a>\u001b[0m encode \u001b[39m=\u001b[39m DeepLab_temp()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000015?line=4'>5</a>\u001b[0m decode \u001b[39m=\u001b[39m Decoder(\u001b[39m'\u001b[39m\u001b[39m5\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000015?line=5'>6</a>\u001b[0m \u001b[39m#elnetwork = ELNetwork(num_classes).cuda()\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Admin\\OneDrive\\C Documents\\GitHub\\ImageClassification\\deeplabv3p_torch\\main.ipynb Cell 9'\u001b[0m in \u001b[0;36mDeepLab_temp.__init__\u001b[1;34m(self, output_stride)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000007?line=36'>37</a>\u001b[0m \u001b[39msuper\u001b[39m(DeepLab_temp, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000007?line=37'>38</a>\u001b[0m BatchNorm \u001b[39m=\u001b[39m SynchronizedBatchNorm2d\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000007?line=39'>40</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackbone \u001b[39m=\u001b[39m ResNet101(output_stride, BatchNorm)\n",
      "\u001b[1;32mc:\\Users\\Admin\\OneDrive\\C Documents\\GitHub\\ImageClassification\\deeplabv3p_torch\\main.ipynb Cell 8'\u001b[0m in \u001b[0;36mResNet101\u001b[1;34m(output_stride, BatchNorm, pretrained)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000006?line=161'>162</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m'''Construct a ResNet-101 model\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000006?line=162'>163</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000006?line=163'>164</a>\u001b[0m \u001b[39m    pretrained(bool): If True, returns a model pre-trained on ImageNet\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000006?line=164'>165</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000006?line=165'>166</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mUsing Resnet101 as a Backbone Network\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000006?line=166'>167</a>\u001b[0m model \u001b[39m=\u001b[39m ResNet(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000006?line=167'>168</a>\u001b[0m     Bottleneck, [\u001b[39m3\u001b[39;49m, \u001b[39m4\u001b[39;49m, \u001b[39m23\u001b[39;49m, \u001b[39m3\u001b[39;49m], output_stride, BatchNorm,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000006?line=168'>169</a>\u001b[0m     pretrained\u001b[39m=\u001b[39;49mpretrained, url\u001b[39m=\u001b[39;49mmodel_urls[\u001b[39m'\u001b[39;49m\u001b[39mresnet101\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000006?line=169'>170</a>\u001b[0m )\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000006?line=170'>171</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "\u001b[1;32mc:\\Users\\Admin\\OneDrive\\C Documents\\GitHub\\ImageClassification\\deeplabv3p_torch\\main.ipynb Cell 8'\u001b[0m in \u001b[0;36mResNet.__init__\u001b[1;34m(self, block, layers, output_stride, BatchNorm, pretrained, url)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000006?line=57'>58</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000006?line=59'>60</a>\u001b[0m \u001b[39m# Modules\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000006?line=60'>61</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mConv2d(\u001b[39m3\u001b[39;49m, \u001b[39m64\u001b[39;49m, stride\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, padding\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, bias\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000006?line=61'>62</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1 \u001b[39m=\u001b[39m BatchNorm(\u001b[39m64\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/deeplabv3p_torch/main.ipynb#ch0000006?line=62'>63</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mReLU(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'kernel_size'"
     ]
    }
   ],
   "source": [
    "num_classes = 5\n",
    "####\n",
    "#set up model (all models are 'constructed at network.modeling)\n",
    "encode = DeepLab_temp()\n",
    "decode = Decoder('5').cuda()\n",
    "#elnetwork = ELNetwork(num_classes).cuda()\n",
    "trainable_params = [\n",
    "    ## main model's parameters\n",
    "    {'params': list(filter(lambda p:p.requires_grad, encode.get_backbone_params())), 'lr':0.0005},\n",
    "    {'params': list(filter(lambda p:p.requires_grad, dec.get_backbone_params())), 'lr':0.005},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (477171462.py, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [9]\u001b[1;36m\u001b[0m\n\u001b[1;33m    metrics =\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = DeepLabv3Plus()\n",
    "\n",
    "\n",
    "\n",
    "#####\n",
    "#set up metrics\n",
    "metrics = \n",
    "    \n",
    "#####\n",
    "#set up optimizer \n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(), lr=config['lr'], weight_decay=config['weight_decay']\n",
    ")\n",
    "if config['lr_policy'] == 'poly':\n",
    "    scheduler = PolyLR\n",
    "elif config['lr_policy'] == 'step':\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=config['step_size'], gamma=0.1)\n",
    "\n",
    "# Gradient scaling 그래디언트가 samll magnitudes에 \n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "######\n",
    "#set up criterion\n",
    "# criterion = utils.get_loss(config['loss_type'])\n",
    "if config['loss_type'] == 'focal_loss':\n",
    "    criterion = utils.FocalLoss(ignore_index = 255, size_average = True)\n",
    "elif config['loss_type'] == 'cross_entropy':\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=255, reduction = 'mean')\n",
    "\n",
    "def save_ckpt(path):\n",
    "    \"\"\"save current model\"\"\"\n",
    "    torch.save()\n",
    "    print(\"Model saved as {}\".format(path))\n",
    "\n",
    "mkdir('checkpoints')\n",
    "\n",
    "# Restore\n",
    "best_score == 0.0\n",
    "cur_itrs = 0\n",
    "cur_epochs = 0\n",
    "if config['ckpt'] not None and os.path.isfile(config['ckpt']): # if checkpoint is available\n",
    "    checkpoint = torch.load(config['ckpt'], map_location = torch.device['cpu'])\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "else:\n",
    "    print(\"[!] Retrain\")\n",
    "    model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "# train Loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('aicomp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca3cd20c8ed94420af4701aa4e5766dbc68c4ea6bc8409f5f338bc464fc401d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
