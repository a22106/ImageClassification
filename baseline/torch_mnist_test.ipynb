{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mnist CNN\n",
    "- layer 1 </br>\n",
    "Conv: in_c = 1, out_c = 32, kernel = 3, stride = 1, padding = 1 </br>\n",
    "ReLU: </br>\n",
    "MaxPool: kernel = 2, stride = 2 </br>\n",
    "\n",
    "- layer 2 </br>\n",
    "Conv: in_c = 32, out_c = 64, kernel = 3, stride = 1, padding = 1 </br>\n",
    "ReLU: </br>\n",
    "MaxPool: kernel = 2, stride = 2 </br>\n",
    "</br>\n",
    "- view </br>\n",
    "FC\n",
    "\n",
    "- Cross Entropy Loss </br>\n",
    "SoftMax </br>\n",
    "NLL Loss\n",
    "\n",
    "\n",
    "- view => (batchsize x [7, 7, 64]) => batch_size x [3136]) </br>\n",
    "Fully_Connect layer => (input = 3136, output = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from models.utils import get_model, EMA\n",
    "from models.deeplabv3.aspp import *\n",
    "from models.deeplabv3 import deeplabv3\n",
    "from modules import datasets\n",
    "from torchvision.models import resnet\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn.init\n",
    "\n",
    "inputs = torch.Tensor(1, 1, 28, 28) # (batch, channel, height, width)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(42)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-4\n",
    "training_epochs = 15\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = dsets.MNIST(root='MNIST_data/', \n",
    "                          train = True,\n",
    "                          transform= transforms.ToTensor(),\n",
    "                          download=True)\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=False,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=mnist_train,\n",
    "                         batch_size = batch_size,\n",
    "                         shuffle = True,\n",
    "                         drop_last = True)\n",
    "test_loader = DataLoader(dataset=mnist_test,\n",
    "                          batch_size= batch_size,\n",
    "                          shuffle= True,\n",
    "                          drop_last= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3,stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3,stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(7*7*64, 10, bias=True) # Fully connected Layer\n",
    "        nn.init.xavier_uniform_(self.fc.weight) # FC 레이어 초기화\n",
    "\n",
    "    def forward(self, x): # x가 out됨\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "\n",
    "        out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5) # chnl-in, out, kernel\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5) \n",
    "        self.fc1 = nn.Linear(1024, 512) # [64*4*4, x]\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)   # 10 classes\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.drop1 = nn.Dropout(0.25)\n",
    "        self.drop2 = nn.Dropout(0.50)\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            self.conv1,     # x in [bs, 1, 28, 28]\n",
    "            nn.ReLU(),      # size([bs, 32, 24, 24])\n",
    "            self.pool,  # size([bs, 32, 12, 12])\n",
    "            self.drop1\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            self.conv2,             # size([bs, 64, 8, 8])\n",
    "            nn.ReLU(),    \n",
    "            self.pool   # size([bs, 64, 4, 4])\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            self.fc1,   # size(bs, 512)\n",
    "            nn.ReLU(),\n",
    "            self.drop2,\n",
    "            self.fc2,   # size(bs, 256)\n",
    "            nn.ReLU(),\n",
    "            self.fc3   # size(bs, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv phase                    \n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "\n",
    "\n",
    "        # neural network phase\n",
    "        out = out.reshape(-1, 1024)    # size([bs, 1024]) \n",
    "        out = self.layer3(out)     \n",
    "        return out\n",
    "\n",
    "def accuracy(model, ds):\n",
    "    ldr = T.utils.data.DataLoader(ds, batch_size=len(ds), shuffle=False)\n",
    "    n_correct = 0\n",
    "    for data in ldr:\n",
    "        (pixels, labels) = data\n",
    "    with T.no_grad():\n",
    "        oupts = model(pixels)\n",
    "        _, predicteds = torch.max(oupts, 1)\n",
    "        n_correct += (predicteds == labels).sum().item()\n",
    "\n",
    "    acc = (n_correct * 1.0) / len(ds)\n",
    "    return acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (drop1): Dropout(p=0.25, inplace=False)\n",
       "  (drop2): Dropout(p=0.5, inplace=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 28, 28])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[8].permute(1, 2, 0))\n",
    "print(y[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1] loss = 0.002301423577591777\n",
      "Learning Finished!\n",
      "[Epoch: 2] loss = 0.002060042694211006\n",
      "Learning Finished!\n",
      "[Epoch: 3] loss = 0.00443555461242795\n",
      "Learning Finished!\n",
      "[Epoch: 4] loss = 0.0023163347505033016\n",
      "Learning Finished!\n",
      "[Epoch: 5] loss = 0.002239817753434181\n",
      "Learning Finished!\n",
      "[Epoch: 6] loss = 0.002327391179278493\n",
      "Learning Finished!\n",
      "[Epoch: 7] loss = 0.0022840392775833607\n",
      "Learning Finished!\n",
      "[Epoch: 8] loss = 0.0023268393706530333\n",
      "Learning Finished!\n",
      "[Epoch: 9] loss = 0.002273962600156665\n",
      "Learning Finished!\n",
      "[Epoch: 10] loss = 0.0023027819115668535\n",
      "Learning Finished!\n",
      "[Epoch: 11] loss = 0.00231882743537426\n",
      "Learning Finished!\n",
      "[Epoch: 12] loss = 0.002242285292595625\n",
      "Learning Finished!\n",
      "[Epoch: 13] loss = 0.0022990726865828037\n",
      "Learning Finished!\n",
      "[Epoch: 14] loss = 0.0023567560128867626\n",
      "Learning Finished!\n",
      "[Epoch: 15] loss = 0.0023155990056693554\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "train_epoch = len(test_loader)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0 # loss를 담음\n",
    "    X, Y = next(iter(test_loader)) # X: image, Y: label\n",
    "    X = X.to(device) # cuda 연산을 하기 위해 필요\n",
    "    Y = Y.to(device)\n",
    "\n",
    "    # Compute prediction and loss\n",
    "    pred = model(X) # 가설(hypothesis) H(x) = Wx + b, 예측값 prediction\n",
    "    loss = criterion(pred, Y) # loss_fn\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    avg_cost += loss / train_epoch\n",
    "    print('[Epoch: {}] loss = {}'.format(epoch+1, avg_cost))\n",
    "    print('Learning Finished!')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =    0 loss = 13749.1517\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "\n",
    "learning_rate = 0.005\n",
    "training_epochs = 50\n",
    "ep_log_interval = 5\n",
    "learning_rate = 0.0005\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss() # does log-softmax()\n",
    "optimizer = torch.optim.SGD(model.parameters(),learning_rate)\n",
    "model.train()\n",
    "for epoch in range(training_epochs):\n",
    "    ep_loss = 0\n",
    "    for (batch_idx, batch) in enumerate(train_loader):\n",
    "        (X, y) = batch # X = pixels, y = target labels\n",
    "        (X, y) = (X.to(device), y.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss_val = criterion(output, y) # a tensor\n",
    "        ep_loss += loss_val.item() # accumulate\n",
    "        loss_val.backward() # compute grads\n",
    "        optimizer.step()\n",
    "    if epoch % ep_log_interval == 0:\n",
    "        print(\"epoch = %4d loss = %0.4f\"%(epoch, ep_loss))\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 511, 511])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = torch.Tensor(5, 3, 511, 511) # b x c x h x w\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "atrouse_layer = nn.Sequential(nn.Conv2d(3, 64, 7, stride=2, padding=3),\n",
    "                              nn.BatchNorm2d(64),\n",
    "                              nn.ReLU(),\n",
    "                              nn.MaxPool2d(3, 2, 1)\n",
    ")\n",
    "\n",
    "project = nn.Sequential(\n",
    "            nn.Conv2d(256, 48, 1, bias=False),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "classifier = nn.Sequential(\n",
    "    nn.Conv2d(304, 256, 3, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(256, 5, 1)\n",
    ")\n",
    "\n",
    "representation = nn.Sequential(\n",
    "    nn.Conv2d(304, 256, 3, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(256, 256, 1)\n",
    ")\n",
    "\n",
    "resnet_m = resnet.resnet101(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 256, 128, 128])\n",
      "prediction: \t\t torch.Size([5, 5, 128, 128])\n",
      "representation: \t torch.Size([5, 256, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "out = atrouse_layer(xx) # 5 x 64 x 128 x 128\n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "#model_v3 = get_model(model_name='deeplabv3p', num_classes=5, output_dim=256)\n",
    "# encoder\n",
    "out_low = resnet_m.layer1(out)  # 5 x 256 x 128 x 128\n",
    "out = resnet_m.layer2(out_low)  # 5 x 512 x 64 x 64\n",
    "out = resnet_m.layer3(out)      # 5 x 1024 x 32 x 32\n",
    "out = resnet_m.layer4(out)      # 5 x 2048 x 16 x 16\n",
    "feature = ASPP(2048, [6, 12, 18])(out) # 5 x 256 x 16 x 16\n",
    "\n",
    "# decoder\n",
    "out_low = project(out_low) # 5 x 48 x 128 x 128\n",
    "# interpolate: 작은 feature의 크기를 크게 변경시킬 때 사용됨(upsampling)\n",
    "output_feature = F.interpolate(feature, size=out_low.shape[2:], mode='bilinear', align_corners=True)\n",
    "# 5 x 256 x 128 x 128\n",
    "print(output_feature.shape)\n",
    "prediction = classifier(torch.cat([out_low, output_feature], dim =1 )) # 5 x 5 x 128 x 128\n",
    "rep = representation(torch.cat([out_low, output_feature], dim=1))  # 5 x 256 x 128 x 128\n",
    "print(\"prediction: \\t\\t\", prediction.shape)\n",
    "print(\"representation: \\t\", rep.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\imgseg\\lib\\site-packages\\torchvision\\transforms\\functional.py:404: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable builtin_function_or_method object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\OneDrive\\C Documents\\GitHub\\ImageClassification\\baseline\\torch_mnist_test.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/OneDrive/C%20Documents/GitHub/ImageClassification/baseline/torch_mnist_test.ipynb#ch0000024?line=0'>1</a>\u001b[0m datasets\u001b[39m.\u001b[39;49mtransform(xx)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\OneDrive\\C Documents\\GitHub\\ImageClassification\\baseline\\modules\\datasets.py:24\u001b[0m, in \u001b[0;36mtransform\u001b[1;34m(image, label, logits, crop_size, scale_size, augmentation)\u001b[0m\n\u001b[0;32m     21\u001b[0m     logits \u001b[39m=\u001b[39m transforms_f\u001b[39m.\u001b[39mresize(logits, (\u001b[39m385\u001b[39m, \u001b[39m513\u001b[39m), Image\u001b[39m.\u001b[39mNEAREST)\n\u001b[0;32m     23\u001b[0m \u001b[39m# Random rescale image\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m raw_w, raw_h \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39msize\n\u001b[0;32m     25\u001b[0m scale_ratio \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39muniform(scale_size[\u001b[39m0\u001b[39m], scale_size[\u001b[39m1\u001b[39m])  \u001b[39m# random scale size (0.8<= i <= 1.0)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m resized_size \u001b[39m=\u001b[39m (\u001b[39mint\u001b[39m(raw_h \u001b[39m*\u001b[39m scale_ratio), \u001b[39mint\u001b[39m(raw_w \u001b[39m*\u001b[39m scale_ratio))\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable builtin_function_or_method object"
     ]
    }
   ],
   "source": [
    "datasets.transform(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "pool = nn.MaxPool2d(2)\n",
    "fc = nn.Linear(3136, 10)\n",
    "relu = nn.ReLU()\n",
    "relu_in = nn.ReLU(True)\n",
    "bn1 = nn.BatchNorm2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (Tensor, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Documents\\GitHub\\ImageClassification\\torch_mnist_test.ipynb Cell 21'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/ImageClassification/torch_mnist_test.ipynb#ch0000019?line=0'>1</a>\u001b[0m out \u001b[39m=\u001b[39m conv1(xx) \u001b[39m# 1, 32, 28 x 28\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/ImageClassification/torch_mnist_test.ipynb#ch0000019?line=1'>2</a>\u001b[0m out \u001b[39m=\u001b[39m bn1(out)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/ImageClassification/torch_mnist_test.ipynb#ch0000019?line=2'>3</a>\u001b[0m out \u001b[39m=\u001b[39m relu_in(out)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/ImageClassification/torch_mnist_test.ipynb#ch0000019?line=4'>5</a>\u001b[0m out \u001b[39m=\u001b[39m pool(out)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\batchnorm.py:130\u001b[0m, in \u001b[0;36m_BatchNorm.__init__\u001b[1;34m(self, num_features, eps, momentum, affine, track_running_stats, device, dtype)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    120\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    121\u001b[0m     num_features,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m     dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[0;32m    128\u001b[0m ):\n\u001b[0;32m    129\u001b[0m     factory_kwargs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m: device, \u001b[39m'\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m'\u001b[39m: dtype}\n\u001b[1;32m--> 130\u001b[0m     \u001b[39msuper\u001b[39m(_BatchNorm, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    131\u001b[0m         num_features, eps, momentum, affine, track_running_stats, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs\n\u001b[0;32m    132\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\batchnorm.py:45\u001b[0m, in \u001b[0;36m_NormBase.__init__\u001b[1;34m(self, num_features, eps, momentum, affine, track_running_stats, device, dtype)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_running_stats \u001b[39m=\u001b[39m track_running_stats\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maffine:\n\u001b[1;32m---> 45\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(num_features, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n\u001b[0;32m     46\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(num_features, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n\u001b[0;32m     47\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (Tensor, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "out = conv1(xx) # 1, 32, 28 x 28\n",
    "out = bn1(out)\n",
    "out = relu_in(out)\n",
    "\n",
    "out = pool(out)\n",
    "out = conv2(out) # 32, 64, 28 x 28\n",
    "out = pool(out) # maxpool 2, b x 64 x 14 x 14\n",
    "out = pool(out) # maxpool 2, b x 64 x 7 x 7\n",
    "out = out.view(out.size(0), -1) # b x 3136\n",
    "out = fc(out)\n",
    "\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000007untitled?line=0'>1</a>\u001b[0m out[\u001b[39m1\u001b[39;49m]\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'annotations' could not be imported from 'c:\\Users\\PiusHwang\\Anaconda3\\envs\\aicomp\\Lib\\site-packages\\torch\\__future__.py'.\n",
      "Click <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "X_train = DataLoader(mnist_train, 5, shuffle=True, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('imgseg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d845fd51fc75d2ae1670e74f4f446c0d06dfba8d4cc9ca0062c740d6c529ae66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
