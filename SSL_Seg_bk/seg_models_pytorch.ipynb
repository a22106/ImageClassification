{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import torch.utils.data.sampler as sampler\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as transforms_f\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from PIL import Image\n",
    "from PIL import ImageFilter\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (729906917.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [31]\u001b[1;36m\u001b[0m\n\u001b[1;33m    class BuildDataLoader:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "root = \"C:\\\\datasets\\\\Aihub\\\\harbor_ship_data\\\\Training\\\\seg\\\\harbor\\\\container_truck_55_source\" #\n",
    "def get_idx(data_dir, train = True, label_num = 4): # if train데이터 idx반환시 True else False\n",
    "    data_dir = os.path.expanduser(root)\n",
    "    if train:\n",
    "        file_name = glob.glob(root + \"/*.jpg\")\n",
    "    print(file_name[:5])\n",
    "\n",
    "    for idx in range(len(file_name)):\n",
    "        file_name[idx] = file_name[idx].split('\\\\')[-1].split('.')[0]\n",
    "    #### 작성중\n",
    "\n",
    "class BuildeDataset(Dataset):\n",
    "    def __init__(self, data_dir, idx_list, crop_size = (512, 512), scale_size = (0.5, 2.0),\n",
    "                 augmentation = True, train = True, apply_partial= None, partial_seed = None):\n",
    "        self.root = os.path.expanduser(data_dir)\n",
    "        self.train = train\n",
    "        self.crop_size = crop_size\n",
    "        self.augmentation = augmentation\n",
    "        self.idx_list = idx_list\n",
    "        self.scale_size = scale_size\n",
    "        self.apply_partial = apply_partial\n",
    "        self.partial_seed = partial_seed\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_root = Image.open(self.data_dir + '/rawImages/{}.jpg'.format(self.idx_list[idx]))\n",
    "        if self.apply_partial is None:\n",
    "            label_root = Image.open(self.root + 'SegClassAug/{}.png'.format(self.idx_list[idx]))\n",
    "\n",
    "class BuildDataLoader:\n",
    "    def __init__(self, num_labels = 4):\n",
    "        self.data_path = \"\"\n",
    "        self.im_size = [513,513]\n",
    "        self.crop_size = [321,321]\n",
    "        self.num_segments = 4 # number of labels num_labels\n",
    "        self.scale_size = (0.5, 0.5)\n",
    "        self.batch_size = 10\n",
    "        self.train_l_idx, self.train_u_idx = get_idx(self.data_path, train = True, label_num = num_labels)\n",
    "        self.test_idx = get_idx(self.data_path, train=False)\n",
    "\n",
    "        if num_labels == 0: # use all data\n",
    "            self.train_l_idx = self.train_u_idx\n",
    "    \n",
    "    def build(self, supervised = False, partial= None, partial_seed= None):\n",
    "        train_l_dataset = BuildDataset()\n",
    "        train_u_dataset = BuildDataset()\n",
    "        test_dataset = BuildDataset()\n",
    "    \n",
    "    #### 작성중\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = BuildDataLoader(num_laebles = 4)\n",
    "train_l_loader, test_loader = data_loader.build(supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'C:\\Users\\Admin\\OneDrive\\C Documents\\GitHub\\reco\\dataset\\pascal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dir = os.path.join(DATA_DIR, 'train')\n",
    "y_train_dir = os.path.join(DATA_DIR, 'trainannot')\n",
    "\n",
    "x_valid_dir = os.path.join(DATA_DIR, 'val')\n",
    "y_valid_dir = os.path.join(DATA_DIR, 'valannot')\n",
    "\n",
    "x_test_dir = os.path.join(DATA_DIR, 'test')\n",
    "y_test_dir = os.path.join(DATA_DIR, 'testannot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolyLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, max_iters, power = 0.9):\n",
    "        self.power\n",
    "        \n",
    "_LRScheduler()\n",
    "scheduler = PolyLR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\datasets\\Aihub\\harbor_ship_data\\Training\\seg\\harbor\\container_truck_55_source\n",
      "['C:\\\\datasets\\\\Aihub\\\\harbor_ship_data\\\\Training\\\\seg\\\\harbor\\\\container_truck_55_source\\\\container_truck_S_55_0000001.jpg', 'C:\\\\datasets\\\\Aihub\\\\harbor_ship_data\\\\Training\\\\seg\\\\harbor\\\\container_truck_55_source\\\\container_truck_S_55_0000002.jpg', 'C:\\\\datasets\\\\Aihub\\\\harbor_ship_data\\\\Training\\\\seg\\\\harbor\\\\container_truck_55_source\\\\container_truck_S_55_0000003.jpg', 'C:\\\\datasets\\\\Aihub\\\\harbor_ship_data\\\\Training\\\\seg\\\\harbor\\\\container_truck_55_source\\\\container_truck_S_55_0000004.jpg', 'C:\\\\datasets\\\\Aihub\\\\harbor_ship_data\\\\Training\\\\seg\\\\harbor\\\\container_truck_55_source\\\\container_truck_S_55_0000005.jpg']\n",
      "['container_truck_S_55_0000001', 'container_truck_S_55_0000002', 'container_truck_S_55_0000003', 'container_truck_S_55_0000004', 'container_truck_S_55_0000005']\n"
     ]
    }
   ],
   "source": [
    "# get indices for labeled, unlabeld\n",
    "root = \"C:\\\\datasets\\\\Aihub\\\\harbor_ship_data\\\\Training\\\\seg\\\\harbor\\\\container_truck_55_source\" #\n",
    "def get_data_idx(root, train = True, label_num = 2):\n",
    "    file_name = []\n",
    "#    for filename in os.istdir(root):\n",
    "    root = os.path.expanduser(root)\n",
    "    print(root) #\n",
    "    \n",
    "    if train:\n",
    "        file_name = glob.glob(root + \"/*.jpg\")\n",
    "    print(file_name[:5])\n",
    "    for idx in range(len(file_name)):\n",
    "        file_name[idx] = file_name[idx].split('\\\\')[-1].split('.')[0]\n",
    "    print(file_name[:5])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "get_data_idx(root)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dced41d100df253a5ef2e3ce847c68478ee9ee572ed365625b465dcce1a06afe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sslseg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
